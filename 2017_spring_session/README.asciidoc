= Infinispan with Spring Boot @ OpenShift Demo

This demo shows how to deploy Infinispan on OpenShift cluster (with Rolling Updates), use Spring integration to put data into the grid, and finally, use Spring Session to store session data between redeployments.

== Slides

Slides for this presentation might be found here: http://redhat.slides.com/slaskawi/infinispan-msa-demo/fullscreen?token=uBeORs7X

== Prerequisites

For running this demo you will need:

* OpenShift Local Cluster (grab the installation files https://github.com/openshift/origin/releases[here])
* https://github.com/johanhaleby/kubetail.git[KubeTail] for observing multiplexed log files from OpenShift
* https://maven.apache.org/[Maven]

== Deploying Infinispan cluster

* Spin up local OpenShift cluster using `oc cluster up`
* Trigger the initialization script `./init_infrastructure.sh`
* Observe Infinispan Server Logs `kubetail -l cluster=old`

Note that Infinispan Servers form a cluster:

```
11:03:43,601 INFO  [org.infinispan.remoting.transport.jgroups.JGroupsTransport] (MSC service thread 1-4) ISPN000094: Received new cluster view for channel clustered: [transactions-repository-1-lbg16|3] (4) [transactions-repository-1-lbg16, transactions-repository-1-j080w, transactions-repository-1-qcw6m, transactions-repository-2-7h02b]
```

== Testing Kubernetes/OpenShift Rolling Updates

Kubernetes/OpenShift Rolling Update is a process of rolling out a new version of application to the production. The progress of this issue has been tracked by JIRA https://issues.jboss.org/browse/ISPN-6673[ISPN-6673].

=== Configuration tuning guide

Tuning configuration might be divided into several steps:

* Tuning JGroups stack to detect failed nodes quickly (but not too fast, otherwise Kubernetes might think that a node experiencing GC pause is dead)
** The easiest way to track GC times is to use `-XX:+PrintGC` in `JAVA_OPTS` environment variable
** On my local PC an averate GC (with CMS enabled) takes ~80ms
** Having said that, an optimized JGroups stack looks like the following

```
<stack name="kubernetes">
  <transport type="TCP" socket-binding="jgroups-tcp"/>
  <!-- Before we begin, we need to know how long does GC take -->
  <!-- On my local machine it's < 0.8s -->
  <protocol type="kubernetes.KUBE_PING">
      <!-- We ping other nodes every 1s -->
      <property name="operationSleep">1000</property>
      <!-- We assume that a node is dead after 3 * 1s = 3s -->
      <!-- This obviously needs to be longer than GC -->
      <property name="operationAttempts">3</property>
  </protocol>
  <protocol type="MERGE3">
      <!-- The min and max intervals need to fit in Liveness probe -->
      <!-- We use 3 * 10s = 30s total -->
      <property name="min_interval">1000</property>
      <property name="max_interval">8000</property>
      <!-- We check for inconsistencies every 10s -->
      <!-- This should play nice with Readiness Probe -->
      <property name="check_interval">10000</property>
  </protocol>
  <protocol type="FD_SOCK" socket-binding="jgroups-tcp-fd"/>
  <protocol type="FD_ALL">
      <!-- The timeout after which we suspect P -->
      <!-- Needs to be much larger than GC -->
      <!-- Let's use 10 * 0.8s (GC) = 2.4s -->
      <property name="timeout">8000</property>
      <!-- How often we check heartbeat messages -->
      <property name="timeout_check_interval">2000</property>
      <!-- How often we send out heartbeats -->
      <property name="interval">3000</property>
  </protocol>
  <!-- If we want to go fast, we need to remove verify suspect -->
  <!-- <protocol type="VERIFY_SUSPECT"/> -->
  <protocol type="pbcast.NAKACK2">
      <property name="use_mcast_xmit">false</property>
  </protocol>
  <protocol type="UNICAST3"/>
  <protocol type="pbcast.STABLE"/>
  <protocol type="pbcast.GMS">
      <property name="view_ack_collection_timeout">9000</property>
  </protocol>
  <protocol type="MFC"/>
  <protocol type="FRAG2"/>
</stack>
```

* Tuning Rolling Update strategy to use HealthCheck API
** Infinispan Docker image contains two scripts: `/usr/local/bin/is_healthy.sh` which return `0` if there's no rebalance in progress and `/usr/local/bin/is_running.sh` which returns `0` if server is running.
** Those scripts shall be used as readiness and liveness probe.
** The Rolling Update procedure should allocate additional Pods first and then slowly destroy the old Infinispan cluster. Also timeouts need to be slightly bigger to make sure we won't lose data because of a longer GC pause:

.Probes configuration
```
livenessProbe:
   exec:
     command:
     - /usr/local/bin/is_running.sh
   initialDelaySeconds: 30
   timeoutSeconds: 30
   periodSeconds: 10
   successThreshold: 1
   failureThreshold: 2
   readinessProbe:
    exec:
       command:
       - /usr/local/bin/is_healthy.sh
    initialDelaySeconds: 30
    timeoutSeconds: 30
    periodSeconds: 10
    successThreshold: 3
    failureThreshold: 2
```

.Rolling Update configuration
```
strategy:
   type: Rolling
   rollingParams:
     updatePeriodSeconds: 20
     intervalSeconds: 20
     timeoutSeconds: 600
     maxUnavailable: 1
     maxSurge: 1
```

** It is very important to prefer longer delays on `initialDelaySeconds` since Kubernetes might start killing not-ready Pods making the rebalance much harder for the cluster!
** Also, during the tests I got better results when using a higher values of `failureThreshold` and `successThreshold`

== Rolling out a new Spring Session app

* Navigate to `transaction-creator` directory and invoke `mvn fabric8:run`. This will load some data into the cluster.
* Navigate to `session-demo` directory and invoke `mvn fabric8:deploy`. This will deploy a Spring Demo app into OpenShift.
* Invoke `watch curl http://session-demo-myproject.192.168.0.17.nip.io/sessions` to see new sessions being created.
* (Optionally) scale out the Spring Demo App by using `oc scale dc session-demo --replicas=3`.
* Do the Rolling Update: `oc deploy session-demo --latest -n myproject`
* Note that all sessions are still there! Magic happens!

== Rolling out a new Infinispan version using Red/Blue deployment

* Do all the steps from previous paragraph.
* Initialize new Infinispan cluster by invoking: `./init_rolling_upgrade_cluster.sh`
* Note that both clusters are SEPARATE!
* Edit service and point it to the new cluster: `oc edit svc transactions-repository`
* Roll out a new Spring App (Spring configuration will pick up new addresses).
* Note that only sessions are present in the new deployment. That's on purpose.
* (Optionally) revert the service configuration and roll back to the old cluster.